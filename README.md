Record my favorite and useful deep learning models encountered in the learning process, including but not limited to classic convolutional neural networks (e.g. AlexNet,ResNet), attention-mechanism related modules, transformer related models (e.g. ViT,Efficient ViT), etc.

 I will continue to update more models as I learn. All models are based on the pytorch implementation and use cuda version 11.8. Another important point is that a large number of implemented models are available in the pytorch framework and transformers library. Reading the source code will improve your coding skills.

The model code has been updated:

+ [image_segmentation：](https://github.com/anshilaoliu/Hand-torn_code/tree/master/image_segmentation)
  + [about_unet：](https://github.com/anshilaoliu/Hand-torn_code/tree/master/image_segmentation/about_unet)
    + [UNet](https://github.com/anshilaoliu/Hand-torn_code/blob/master/image_segmentation/about_unet/UNet.py)
    + [UNet_pp](https://github.com/anshilaoliu/Hand-torn_code/blob/master/image_segmentation/about_unet/UNet_pp.py)
+ [about_transformer:](https://github.com/anshilaoliu/Hand-torn_code/tree/master/about_transformer)
  + [Transformer](https://github.com/anshilaoliu/Hand-torn_code/tree/master/about_transformer/attention_is_all_you_need)
  + [ViT_model](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_transformer/ViT/ViT_model.py)
  + [mobile_vit](https://github.com/anshilaoliu/Hand-torn_code/tree/master/about_transformer/mobile_vit)
+ [about_attention：](https://github.com/anshilaoliu/Hand-torn_code/tree/master/about_attention)
  + [MSCAAttention](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_attention/MSCAAttention.py)
  + [CBAM](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_attention/CBAM.py)
+ [classic_conv：](https://github.com/anshilaoliu/Hand-torn_code/tree/master/classic_conv)
  + [AlexNet](https://github.com/anshilaoliu/Hand-torn_code/blob/master/classic_conv/AlexNet.py)
+ [about_interview](https://github.com/anshilaoliu/Hand-torn_code/tree/master/about_interview)
  + [Attention](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_interview/Attention.py)
  + [Embedding](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_interview/Embedding.py)
  + [FFN](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_interview/FFN.py)
  + [LayerNorm](https://github.com/anshilaoliu/Hand-torn_code/blob/master/about_interview/LayerNorm.py)

